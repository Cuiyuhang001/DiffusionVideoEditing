# DiffusionVideoEditing
Official project repo for paper "Speech Driven Video Editing via an Audio-Conditioned Diffusion Model" 

Shoutout to https://github.com/Janspiry/Palette-Image-to-Image-Diffusion-Models ! Most of the code in this repo is taken from there. It's a really good implementation of the Palette image2image paper, so go check it out! 

You can check out some of the results on the project page found here: https://danbigioi.github.io/DiffusionVideoEditing/

And if you want to read the paper, this is the link to the arxiv submission: https://arxiv.org/abs/2301.04474 

Note that while the work presented in this paper is relatively early stage stuff, i'll be using this repo as a "base" of sorts where I'll be periodically updating the model and implementation with newer approaches im currently working on. 

# TO DO: 

Write some tutorials for how to use the code (I apologise its genuinely a mess, I need to streamline it a bit when I get the time). It's on my todo list to write out a detailed guide for both training and running inference. Also need to upload the trained model weights. 

Upload datasets. I'll see if I can somehow upload the whole processed dataset here, but most likely I'll upload the preprocessing scripts I made with a guide on          how to extract the stuff you need. For reference, I am using the GRID dataset for a lot of these experiments, but in theory the model should work with any                audiovisual dataset like TCD Timit, Lombard Grid, etc.  


