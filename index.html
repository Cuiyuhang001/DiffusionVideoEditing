<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Project page for the paper "Speech Driven Video Editing via an Audio-Conditioned Diffusion Model">
  <meta property="og:title" content="Speech-Driven Video Editing Via Diffusion Models"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="https://danbigioi.github.io/DiffusionVideoEditing/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Speech Driven Video Editing via an Audio-Conditioned Diffusion Model</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Speech Driven Video Editing via an Audio-Conditioned Diffusion Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Dan Bigioi<sup>1</sup>,</span>
                <span class="author-block">
                  Shubhajit Basak<sup>1</sup>,</span>
                 <span class="author-block">
                    Michał Stypułkowski<sup>2</sup>,</span>
                  <span class="author-block">
                    Maciej Zieba<sup>3,4</sup>,</span>
                  <span class="author-block">
                    Hugh Jordan<sup>5</sup>,</span>
                      <span class="author-block">
                        <a href="https://www.scss.tcd.ie/Rachel.McDonnell/">Rachel McDonnell</a><sup>5</sup>,</span>
                          <span class="author-block">
                            <a href="https://www.universityofgalway.ie/our-research/people/engineering-and-informatics/petercorcoran/">Peter Corcoran</a><sup>1</sup>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">2023</span>
                    <span class="eql-cntrb"><small><br><sup>1</sup>University of Galway</small></span>
                    <span class="eql-cntrb"><small><br><sup>2</sup>University of Wrocław</small></span>
                    <span class="eql-cntrb"><small><br><sup>3</sup>Wrocław University of Science and Technology</small></span>
                    <span class="eql-cntrb"><small><br><sup>4</sup>Tooploox</small></span>
                    <span class="eql-cntrb"><small><br><sup>5</sup>Trinity College Dublin</small></span>
                                           
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2301.04474" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/DanBigioi/DiffusionVideoEditing" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2301.04474" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/multispeaker_videos/MultiSpeakerDemo.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
         Sample video results generated by our multispeaker model on unseen subjects
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
              Taking inspiration from recent developments in visual generative tasks using diffusion models, we propose a method for end-to-end speech-driven video editing using a denoising diffusion model. Given a video of a talking person, and a separate auditory speech recording, the lip and jaw motions are re-synchronized without relying on intermediate structural representations such as facial landmarks or a 3D face model. We show this is possible by conditioning a denoising diffusion model on audio mel spectral features to generate synchronised facial motion. Proof of concept results are demonstrated on both single-speaker and multi-speaker video editing, providing a baseline model on the CREMA-D audiovisual data set. To the best of our knowledge, this is the first work to demonstrate and validate the feasibility of applying end-to-end denoising diffusion models to the task of audio-driven video editing.
          </p>
        </div>
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/network_diagram.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
           High-Level Overview of Network Architecture including the forward and backward diffusion processes. 
        </h2>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3"> Videos Generated By Our Multi-Speaker Model with Unseen Identities </h2>
      <div class="content has-text-justified">
          <p>
            Note, that while these results look really nice, the models still suffer from a lack of prolonged training time due to limitations in our available hardware. We recommend users with access to beefier GPUs to continue training from our pretrained checkpoints! More details regarding the training set up available in the paper :) Don't forget to click the arrow icon to cycle through the videos! 
          </p>
        </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" controls loop height="100%">
            <!-- Your video file here -->
            <source src="static/multispeaker_videos/1052_IWW_ANG_XX.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            "I wonder what this is about"
          </h2>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" controls  loop height="100%">
            <!-- Your video file here -->
            <source src="static/multispeaker_videos/1015_IEO_ANG_HI.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            "Its eleven o clock"
          </h2>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" controls loop height="100%">
            <!-- Your video file here -->
            <source src="static/multispeaker_videos/1081_TIE_HAP_XX.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            "That is exactly what happened"
          </h2>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" controls loop height="100%">
            <!-- Your video file here -->
            <source src="static/multispeaker_videos/1020_TSI_FEA_XX.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            "The surface is slick"
          </h2>
        </div>
        <div class="item item-video5">
          <video poster="" id="video5" controls loop height="100%">
            <!-- Your video file here -->
            <source src="static/multispeaker_videos/1030_TIE_DIS_XX.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            "That is exactly what happened"
          </h2>
        </div>
        <div class="item item-video6">
          <video poster="" id="video6" controls loop height="100%">\
            <!-- Your video file here -->
            <source src="static/multispeaker_videos/1082_ITS_ANG_XX.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            "I think Ive seen this before"
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->
  
 <!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3"> Test Samples Generated by SingleSpeaker Model</h2>
      <p>
        The following are videos generated by our single speaker model from the unseen test set. 
      </p>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" controls loop height="100%">
            <!-- Your video file here -->
            <source src="static/single_speaker_videos/SingleSpeakerVideos.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            This model does not use attention within th up/downsampling layers of the unet in order to save on computational cost. For better results, enable it!  
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<!-- Paper PDF -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Paper PDF</h2>

      <iframe  src="static/pdfs/Arxiv_Submission.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{https://doi.org/10.48550/arxiv.2301.04474,
  doi = {10.48550/ARXIV.2301.04474},
  
  url = {https://arxiv.org/abs/2301.04474},
  
  author = {Bigioi, Dan and Basak, Shubhajit and Jordan, Hugh and McDonnell, Rachel and Corcoran, Peter},
  
  title = {Speech Driven Video Editing via an Audio-Conditioned Diffusion Model},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
