<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Project page for the paper "Speech Driven Video Editing via an Audio-Conditioned Diffusion Model">
  <meta property="og:title" content="Speech-Driven Video Editing Via Diffusion Models"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="https://danbigioi.github.io/DiffusionVideoEditing/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Speech Driven Video Editing via an Audio-Conditioned Diffusion Model</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Speech Driven Video Editing via an Audio-Conditioned Diffusion Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Dan Bigioi<sup>1</sup>,</span>
                <span class="author-block">
                  Shubhajit Basak<sup>1</sup>,</span>
                  <span class="author-block">
                    Hugh Jordan<sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="https://www.scss.tcd.ie/Rachel.McDonnell/">Rachel McDonnell</a><sup>2</sup>,</span>
                          <span class="author-block">
                            <a href="https://www.universityofgalway.ie/our-research/people/engineering-and-informatics/petercorcoran/">Peter Corcoran</a><sup>1</sup>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">2023</span>
                    <span class="eql-cntrb"><small><br><sup>1</sup>University of Galway</small></span>
                    <span class="eql-cntrb"><small><br><sup>2</sup>Trinity College Dublin</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2301.04474" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/DanBigioi/DiffusionVideoEditing" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2301.04474" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/multispeaker_videos/MultiSpeakerDemo.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
         Sample video generated by our single speaker model 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
              Taking inspiration from recent developments in visual generative tasks using diffusion models, we propose a method for end-to-end speech-driven video editing using a denoising diffusion model. Given a video of a talking person, and a separate auditory speech recording, the lip and jaw motions are re-synchronized without relying on intermediate structural representations such as facial landmarks or a 3D face model. We show this is possible by conditioning a denoising diffusion model on audio mel spectral features to generate synchronised facial motion. Proof of concept results are demonstrated on both single-speaker and multi-speaker video editing, providing a baseline model on the CREMA-D audiovisual data set. To the best of our knowledge, this is the first work to demonstrate and validate the feasibility of applying end-to-end denoising diffusion models to the task of audio-driven video editing.
          </p>
        </div>
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/network_diagram.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
           High-Level Overview of Network Architecture including the forward and backward diffusion processes. 
        </h2>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Test Samples Generated By Our Single Speaker Model</h2>
      <div class="content has-text-justified">
          <p>
            Note, that while these results are promising, the models suffer from a lack of prolonged training time due to limitations in our available hardware. We recommend users with access to beefier GPUs to train their own models from scratch! More details regarding the training set up available in the paper :) Don't forget to click the arrow icon to cycle through the videos! 
          </p>
        </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" controls loop height="100%">
            <!-- Your video file here -->
            <source src="static/singlespeaker_videos/bgia5a.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            "Bin Green In A Five Again"
          </h2>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" controls  loop height="100%">
            <!-- Your video file here -->
            <source src="static/singlespeaker_videos/swwv6n.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            "Set White With V Six Now"
          </h2>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" controls loop height="100%">
            <!-- Your video file here -->
            <source src="static/singlespeaker_videos/brbg3s.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            "Bin Red By G Three Soon"
          </h2>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" controls loop height="100%">
            <!-- Your video file here -->
            <source src="static/singlespeaker_videos/lbix6p.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            "Lay Blue In X Six Please"
          </h2>
        </div>
        <div class="item item-video5">
          <video poster="" id="video5" controls loop height="100%">
            <!-- Your video file here -->
            <source src="static/singlespeaker_videos/srbo3s.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            "Set Red By O Three Soon"
          </h2>
        </div>
        <div class="item item-video6">
          <video poster="" id="video6" controls loop height="100%">\
            <!-- Your video file here -->
            <source src="static/singlespeaker_videos/prbp8n.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            "Place Red by P Eight Now"
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->
  
 <!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3"> Test Samples Generated by MultiSpeaker Model</h2>
      <p>
        The following are a collection of videos our network modifies from the unseen test set. The quality of the videos is poorer than that of the single-speaker model A) A lack of sufficient training, B) Trained on only 10% available dataset, C) and most importantly, the lack of an "identity" frame, whose ommission causes the rapid degradation in speaker identity the more frames are generated. Nonetheless, these proof of concept results show promise! 
      </p>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" controls loop height="100%">
            <!-- Your video file here -->
            <source src="static/multispeaker_videos/Part_1.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            A combination of not enough training, and lack of identity frame input to the model, are most liklely the leading causes for poorer performance! 
          </h2>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" controls loop height="100%">
            <!-- Your video file here -->
            <source src="static/multispeaker_videos/Part_2.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" controls loop height="100%">
            <!-- Your video file here -->
            <source src="static/multispeaker_videos/Part_3.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" controls loop height="100%">
            <!-- Your video file here -->
            <source src="static/multispeaker_videos/Part_4.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<!-- Paper PDF -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Paper PDF</h2>

      <iframe  src="static/pdfs/Arxiv_Submission.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{https://doi.org/10.48550/arxiv.2301.04474,
  doi = {10.48550/ARXIV.2301.04474},
  
  url = {https://arxiv.org/abs/2301.04474},
  
  author = {Bigioi, Dan and Basak, Shubhajit and Jordan, Hugh and McDonnell, Rachel and Corcoran, Peter},
  
  title = {Speech Driven Video Editing via an Audio-Conditioned Diffusion Model},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
